{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_bbox(points_list, img_width, img_height):\n",
    "    points = [tuple(map(float, point.split(','))) for point in points_list]\n",
    "\n",
    "    # Find extremes\n",
    "    min_x = min(point[0] for point in points)\n",
    "    max_x = max(point[0] for point in points)\n",
    "    min_y = min(point[1] for point in points)\n",
    "    max_y = max(point[1] for point in points)\n",
    "\n",
    "    # Calculate bbox center, width, and height\n",
    "    x_center = (min_x + max_x) / 2\n",
    "    y_center = (min_y + max_y) / 2\n",
    "    width = max_x - min_x\n",
    "    height = max_y - min_y\n",
    "\n",
    "    # Normalize values\n",
    "    x_center_norm = x_center / float(img_width)\n",
    "    y_center_norm = y_center / float(img_height)\n",
    "    width_norm = width / float(img_width)\n",
    "    \n",
    "    height_norm = height / float(img_height)\n",
    "\n",
    "    return x_center_norm, y_center_norm, width_norm, height_norm\n",
    "    # return x_center, y_center, width, height"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "images/1.png not_free_parking_space ['581.38,53.07', '630.34,37.53', '650.00,97.25', '650.00,130.92', '610.60,142.80']\n",
      "images/1.png not_free_parking_space ['0.00,322.96', '48.27,309.83', '76.90,405.00', '23.60,420.70', '0.00,339.00']\n",
      "images/1.png not_free_parking_space ['528.14,398.26', '579.24,382.81', '610.13,478.75', '581.63,487.00', '557.23,486.95']\n",
      "images/1.png not_free_parking_space ['579.46,382.58', '630.56,367.12', '650.00,427.57', '650.00,466.40', '610.70,478.50']\n",
      "images/10.png free_parking_space ['2375.63,484.05', '2560.00,486.89', '2560.00,525.89', '2558.69,986.39', '2379.89,987.81']\n",
      "images/13.png not_free_parking_space ['0.00,39.68', '59.78,38.71', '60.00,161.10', '0.00,164.50', '0.00,90.39']\n",
      "images/14.png not_free_parking_space ['1589.60,617.70', '1891.35,621.39', '1920.00,921.14', '1919.78,1079.53', '1624.81,1080.00']\n",
      "images/14.png not_free_parking_space ['979.70,620.37', '1275.20,617.30', '1304.30,1080.00', '1285.09,1080.00', '988.55,1078.92']\n",
      "images/15.png not_free_parking_space ['77.29,516.63', '193.31,517.18', '158.88,732.00', '81.64,732.00', '25.61,731.07']\n",
      "images/15.png not_free_parking_space ['202.93,518.88', '314.56,516.68', '293.13,732.00', '266.03,732.00', '162.79,731.13']\n",
      "images/15.png not_free_parking_space ['321.15,519.43', '437.72,518.88', '429.55,732.00', '319.61,732.00', '300.81,731.68']\n",
      "images/15.png not_free_parking_space ['916.10,517.23', '1025.52,515.58', '1100.00,721.95', '1100.00,732.00', '972.34,732.00']\n",
      "images/15.png not_free_parking_space ['16.53,73.17', '100.39,73.17', '64.19,182.68', '0.00,184.91', '0.00,174.01']\n",
      "images/15.png free_parking_space ['1017.00,191.70', '1095.31,191.71', '1100.00,201.96', '1100.00,320.65', '1063.69,320.47']\n",
      "images/17.png not_free_parking_space ['484.42,0.00', '485.20,136.50', '248.85,136.91', '249.65,0.70', '386.35,0.00']\n",
      "images/22.png not_free_parking_space ['119.90,118.40', '193.20,116.80', '193.00,0.70', '125.61,0.11', '125.61,0.11', '124.40,0.10']\n",
      "images/22.png not_free_parking_space ['53.30,120.80', '118.10,118.70', '122.50,0.60', '55.70,0.00', '55.70,0.00', '55.70,0.00']\n",
      "images/22.png not_free_parking_space ['342.80,113.70', '419.20,113.30', '420.10,0.00', '368.81,0.00', '343.50,0.00']\n",
      "images/22.png not_free_parking_space ['420.75,113.95', '497.15,113.55', '498.05,0.25', '446.76,0.25', '421.45,0.25']\n",
      "images/22.png not_free_parking_space ['498.38,113.95', '577.50,112.70', '573.60,0.00', '524.39,0.25', '499.08,0.25']\n",
      "images/26.png not_free_parking_space ['1129.71,625.33', '1195.16,624.09', '1200.00,648.57', '1200.00,715.08', '1150.86,716.65']\n",
      "images/26.png not_free_parking_space ['192.20,190.90', '256.30,191.80', '259.99,287.13', '192.59,287.13', '193.42,287.13']\n",
      "images/26.png free_parking_space ['454.92,195.46', '519.72,197.01', '531.64,289.29', '462.90,288.90', '464.08,288.77']\n",
      "images/26.png not_free_parking_space ['1137.54,215.12', '1190.91,216.57', '1200.00,250.86', '1200.00,302.06', '1164.35,303.99']\n",
      "images/27.png free_parking_space ['407.59,61.90', '410.04,61.99', '468.88,62.31', '473.89,179.68', '412.92,178.64', '413.60,179.30']\n",
      "images/27.png not_free_parking_space ['471.38,63.35', '531.40,63.60', '536.60,180.70', '473.89,179.68', '475.29,180.20']\n",
      "images/27.png not_free_parking_space ['593.45,63.09', '654.74,63.09', '659.70,180.98', '598.10,181.00', '600.23,181.24']\n",
      "images/28.png not_free_parking_space ['100.50,389.90', '102.93,565.66', '68.17,566.00', '0.00,566.00', '0.00,389.04']\n",
      "images/29.png not_free_parking_space ['372.25,1.42', '421.69,1.66', '421.70,82.40', '370.81,81.68', '372.72,81.92']\n",
      "images/29.png not_free_parking_space ['113.51,143.38', '114.86,143.81', '164.45,143.81', '163.88,224.65', '115.72,223.79']\n",
      "images/3.png free_parking_space ['211.70,97.60', '264.23,98.45', '260.40,97.80', '277.96,173.71', '224.90,173.71']\n",
      "images/3.png not_free_parking_space ['0.20,96.70', '46.50,96.00', '36.59,19.93', '0.00,20.48', '0.00,85.47']\n",
      "images/3.png not_free_parking_space ['196.93,20.92', '245.76,20.26', '259.61,87.23', '261.59,95.15', '210.50,95.50']\n",
      "images/3.png not_free_parking_space ['316.70,98.50', '368.80,96.10', '385.31,172.35', '332.86,173.34', '334.18,173.67']\n",
      "images/3.png free_parking_space ['523.90,96.80', '567.80,97.10', '588.87,170.04', '540.37,171.69', '541.03,170.70', '541.36,170.70']\n",
      "images/3.png free_parking_space ['131.48,267.04', '187.23,266.38', '202.08,352.82', '142.36,355.45', '144.67,355.12']\n",
      "images/30.png free_parking_space ['842.60,53.60', '885.87,56.96', '890.00,69.32', '890.00,121.27', '862.75,120.63']\n",
      "images/30.png not_free_parking_space ['689.72,199.90', '697.78,201.69', '692.41,200.50']\n",
      "images/5.png free_parking_space ['651.40,20.50', '672.10,73.32', '712.00,72.34', '712.00,47.90', '700.10,19.49']\n",
      "images/5.png not_free_parking_space ['13.67,161.24', '72.77,160.38', '52.10,234.60', '0.00,235.46', '0.00,204.05']\n",
      "images/5.png free_parking_space ['624.90,152.10', '691.70,150.90', '712.00,209.05', '712.00,224.97', '649.70,226.90']\n",
      "images/5.png free_parking_space ['8.01,236.32', '70.55,235.20', '49.99,319.89', '-0.00,319.47', '0.00,264.76']\n",
      "images/5.png not_free_parking_space ['89.71,445.94', '4.61,446.44', '0.00,464.86', '0.00,572.80', '64.08,574.09']\n",
      "images/6.png free_parking_space ['984.90,1.00', '1049.90,52.00', '1077.20,34.50', '1030.71,0.00', '994.78,0.00']\n",
      "images/6.png free_parking_space ['1253.95,557.96', '1214.76,454.28', '1262.66,450.34', '1280.00,490.73', '1280.00,556.35']\n",
      "images/6.png free_parking_space ['1087.74,404.75', '1067.35,423.24', '1046.81,424.97', '977.30,354.10', '1007.81,331.92']\n",
      "images/6.png not_free_parking_space ['1264.60,246.61', '1280.00,233.96', '1280.00,217.10', '1216.10,162.10', '1186.13,186.02']\n",
      "images/6.png not_free_parking_space ['51.80,307.49', '0.00,332.72', '0.00,347.11', '45.61,421.90', '106.59,389.15']\n",
      "images/6.png free_parking_space ['690.50,68.80', '723.80,49.60', '672.90,0.00', '662.02,0.00', '636.00,13.60']\n",
      "images/8.png not_free_parking_space ['524.30,263.90', '596.21,294.07', '612.00,263.64', '612.00,149.27', '578.14,139.37']\n",
      "images/8.png not_free_parking_space ['12.26,305.66', '86.79,339.16', '14.89,484.99', '0.00,477.52', '0.00,330.70']\n",
      "images/8.png free_parking_space ['492.12,701.44', '571.28,737.24', '612.00,641.38', '612.00,551.68', '562.40,535.90']\n",
      "images/8.png free_parking_space ['145.00,106.20', '215.43,133.66', '278.60,19.00', '235.95,0.00', '197.92,0.00']\n",
      "images/9.png not_free_parking_space ['582.17,17.88', '676.04,0.00', '685.68,0.00', '740.10,102.60', '629.40,131.10']\n",
      "images/9.png partially_free_parking_space ['943.17,833.93', '1258.92,720.80', '1252.16,727.55', '1304.50,820.42', '985.38,935.24']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "\n",
    "with open('annotations.xml', 'r') as f:\n",
    "    data = f.read()\n",
    "\n",
    "mapping = {'free_parking_space' : 0,'not_free_parking_space' : 1,'partially_free_parking_space' : 2}\n",
    "\n",
    "Bs_data = BeautifulSoup(data, \"lxml\")\n",
    "image_polygons = {}\n",
    "b_unique = Bs_data.find_all('image')\n",
    "for image in b_unique:\n",
    "    image_name = image.get('name')\n",
    "    img_height = image.get('height')\n",
    "    img_width = image.get('width')\n",
    "    polygons = []\n",
    "    points_data = image.find_all(\"polygon\")\n",
    "    for polygon in points_data:\n",
    "        points = polygon.get('points').split(';')\n",
    "        if len(points) != 4:\n",
    "            print(image_name, polygon.get('label'), points)\n",
    "        bbox_values = calculate_bbox(points, img_width, img_height)\n",
    "        xcent, ycent, boxW, boxH = bbox_values\n",
    "\n",
    "        polygons.append([mapping[polygon.get('label')]] + [xcent, ycent, boxW, boxH])\n",
    "        # polygons.append([mapping[polygon.get('label')]] + [0.5, 0.5, 0.5, 0.5])\n",
    "# \n",
    "    image_polygons[image_name] = polygons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "label_root_path = 'archive/labels'\n",
    "os.makedirs(label_root_path, exist_ok=True)\n",
    "\n",
    "for image_name, polygons in image_polygons.items():\n",
    "    file_path = os.path.join(label_root_path, image_name.replace(\".png\", \"\").replace(\"images/\", \"\") + '.txt')\n",
    "    with open(file_path, 'w') as file:\n",
    "        for polygon in polygons:\n",
    "            file.write(\" \".join([str(x) for x in polygon])+\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copied 24 images to archive/working/Train/images\n",
      "Copied 6 images to archive/working/val/images\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Define the paths\n",
    "images_path = \"archive/images/\"\n",
    "labels_path = \"archive/labels/\"\n",
    "\n",
    "train_img_path = \"archive/working/Train/images\"\n",
    "test_img_path = \"archive/working/val/images\"\n",
    "val_img_path = \"archive/working/Test/images\"\n",
    "train_label_path = \"archive/working/Train/labels\"\n",
    "test_label_path = \"archive/working/val/labels\"\n",
    "val_label_path = \"archive/working/Test/labels\"\n",
    "\n",
    "\n",
    "for pth in [train_img_path, test_img_path, val_img_path, train_label_path, test_label_path, val_label_path]:\n",
    "    os.makedirs(pth, exist_ok=True)\n",
    "\n",
    "# List all images in the directory\n",
    "all_images = [f for f in os.listdir(images_path) if os.path.isfile(os.path.join(images_path, f))]\n",
    "\n",
    "# Split images into training and testing sets\n",
    "train_images, test_images = train_test_split(all_images, test_size=0.2, random_state=42)\n",
    "\n",
    "# Function to copy images to the respective folder\n",
    "def copy_files(file_list, source_dir, dest_dir):\n",
    "    for file_name in file_list:\n",
    "        # Image\n",
    "        source = os.path.join(source_dir, file_name)\n",
    "        destination = os.path.join(dest_dir, file_name)\n",
    "        shutil.copy2(source, destination)\n",
    "        # Label\n",
    "        source = source.replace(\"images\", \"labels\").replace(\".png\", \".txt\")\n",
    "        destination = destination.replace(\"images\", \"labels\").replace(\".png\", \".txt\")\n",
    "        shutil.copy2(source, destination)\n",
    "\n",
    "# Copy train images\n",
    "copy_files(train_images, images_path, train_img_path)\n",
    "# Copy test images\n",
    "copy_files(test_images, images_path, test_img_path)\n",
    "\n",
    "print(f\"Copied {len(train_images)} images to {train_img_path}\")\n",
    "print(f\"Copied {len(test_images)} images to {test_img_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -q git+https://github.com/THU-MIG/yolov10.git\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !mkdir -p archive/weights\n",
    "# !wget -P archive/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10n.pt\n",
    "# !wget -P archive/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10s.pt\n",
    "# !wget -P archive/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10m.pt\n",
    "# !wget -P archive/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10b.pt\n",
    "# !wget -P archive/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10x.pt\n",
    "# !wget -P archive/weights -q https://github.com/jameslahm/yolov10/releases/download/v1.0/yolov10l.pt\n",
    "# !ls -lh archive/weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New https://pypi.org/project/ultralytics/8.2.26 available 😃 Update with 'pip install -U ultralytics'\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.12.2 torch-2.3.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "\u001b[34m\u001b[1mengine\\trainer: \u001b[0mtask=detect, mode=train, model=archive/weights/yolov10x.pt, data=data.yaml, epochs=150, time=None, patience=100, batch=32, imgsz=640, save=True, save_period=-1, val_period=1, cache=False, device=None, workers=8, project=None, name=train, exist_ok=False, pretrained=True, optimizer=AdamW, verbose=True, seed=0, deterministic=True, single_cls=False, rect=False, cos_lr=False, close_mosaic=10, resume=False, amp=True, fraction=1.0, profile=False, freeze=None, multi_scale=False, overlap_mask=True, mask_ratio=4, dropout=0.0, val=True, split=val, save_json=False, save_hybrid=False, conf=None, iou=0.7, max_det=300, half=False, dnn=False, plots=True, source=None, vid_stride=1, stream_buffer=False, visualize=False, augment=False, agnostic_nms=False, classes=None, retina_masks=False, embed=None, show=False, save_frames=False, save_txt=False, save_conf=False, save_crop=False, show_labels=True, show_conf=True, show_boxes=True, line_width=None, format=torchscript, keras=False, optimize=False, int8=False, dynamic=False, simplify=False, opset=None, workspace=4, nms=False, lr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0001, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=7.5, cls=0.5, dfl=1.5, pose=12.0, kobj=1.0, label_smoothing=0.0, nbs=64, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, bgr=0.0, mosaic=1.0, mixup=0.0, copy_paste=0.0, auto_augment=randaugment, erasing=0.4, crop_fraction=1.0, cfg=None, tracker=botsort.yaml, save_dir=runs\\detect\\train\n",
      "Overriding model.yaml nc=80 with nc=3\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1      2320  ultralytics.nn.modules.conv.Conv             [3, 80, 3, 2]                 \n",
      "  1                  -1  1    115520  ultralytics.nn.modules.conv.Conv             [80, 160, 3, 2]               \n",
      "  2                  -1  3    436800  ultralytics.nn.modules.block.C2f             [160, 160, 3, True]           \n",
      "  3                  -1  1    461440  ultralytics.nn.modules.conv.Conv             [160, 320, 3, 2]              \n",
      "  4                  -1  6   3281920  ultralytics.nn.modules.block.C2f             [320, 320, 6, True]           \n",
      "  5                  -1  1    213120  ultralytics.nn.modules.block.SCDown          [320, 640, 3, 2]              \n",
      "  6                  -1  6   4604160  ultralytics.nn.modules.block.C2fCIB          [640, 640, 6, True]           \n",
      "  7                  -1  1    417920  ultralytics.nn.modules.block.SCDown          [640, 640, 3, 2]              \n",
      "  8                  -1  3   2712960  ultralytics.nn.modules.block.C2fCIB          [640, 640, 3, True]           \n",
      "  9                  -1  1   1025920  ultralytics.nn.modules.block.SPPF            [640, 640, 5]                 \n",
      " 10                  -1  1   1545920  ultralytics.nn.modules.block.PSA             [640, 640]                    \n",
      " 11                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 12             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 13                  -1  3   3122560  ultralytics.nn.modules.block.C2fCIB          [1280, 640, 3, True]          \n",
      " 14                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 15             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 16                  -1  3   1948800  ultralytics.nn.modules.block.C2f             [960, 320, 3]                 \n",
      " 17                  -1  1    922240  ultralytics.nn.modules.conv.Conv             [320, 320, 3, 2]              \n",
      " 18            [-1, 13]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 19                  -1  3   2917760  ultralytics.nn.modules.block.C2fCIB          [960, 640, 3, True]           \n",
      " 20                  -1  1    417920  ultralytics.nn.modules.block.SCDown          [640, 640, 3, 2]              \n",
      " 21            [-1, 10]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 22                  -1  3   3122560  ultralytics.nn.modules.block.C2fCIB          [1280, 640, 3, True]          \n",
      " 23        [16, 19, 22]  1   4390818  ultralytics.nn.modules.head.v10Detect        [3, [320, 640, 640]]          \n",
      "YOLOv10x summary: 688 layers, 31660658 parameters, 31660642 gradients, 171.0 GFLOPs\n",
      "\n",
      "Transferred 1123/1135 items from pretrained weights\n",
      "Freezing layer 'model.23.dfl.conv.weight'\n",
      "Plotting labels to runs\\detect\\train\\labels.jpg... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.01, momentum=0.937) with parameter groups 185 weight(decay=0.0), 198 weight(decay=0.0001), 197 bias(decay=0.0)\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mlogging run_id(f98a5f4e61bd4a659246be6de41a9889) to runs\\mlflow\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mview at http://127.0.0.1:5000 with 'mlflow server --backend-store-uri runs\\mlflow'\n",
      "\u001b[34m\u001b[1mMLflow: \u001b[0mdisable with 'yolo settings mlflow=False'\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1mruns\\detect\\train\u001b[0m\n",
      "Starting training for 150 epochs...\n",
      "\n",
      "      Epoch    GPU_mem     box_om     cls_om     dfl_om     box_oo     cls_oo     dfl_oo  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\working\\Train\\labels.cache... 24 images, 0 backgrounds, 0 corrupt: 100%|██████████| 24/24 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\working\\Train\\labels.cache... 24 images, 0 backgrounds, 0 corrupt: 100%|██████████| 24/24 [00:00<?, ?it/s]\n",
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\working\\val\\labels.cache... 6 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\working\\val\\labels.cache... 6 images, 0 backgrounds, 0 corrupt: 100%|██████████| 6/6 [00:00<?, ?it/s]\n",
      "\n",
      "  0%|          | 0/1 [00:00<?, ?it/s]\n"
     ]
    }
   ],
   "source": [
    "!yolo task=detect mode=train epochs=150 batch=32 plots=True model=archive/weights/yolov10x.pt data=data.yaml optimizer=AdamW weight_decay=0.0001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics YOLOv8.1.34 🚀 Python-3.12.2 torch-2.3.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "YOLOv10x summary (fused): 503 layers, 31738160 parameters, 185184 gradients, 170.6 GFLOPs\n",
      "\n",
      "image 1/1 C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\images\\0.png: 352x640 19 67s, 1276.8ms\n",
      "Speed: 8.8ms preprocess, 1276.8ms inference, 23.0ms postprocess per image at shape (1, 3, 352, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict19\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.12.2 torch-2.3.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "YOLOv10x summary (fused): 503 layers, 31738160 parameters, 185184 gradients, 170.6 GFLOPs\n",
      "\n",
      "image 1/1 C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\images\\1.png: 480x640 1 2, 18 67s, 2164.5ms\n",
      "Speed: 5.4ms preprocess, 2164.5ms inference, 5.5ms postprocess per image at shape (1, 3, 480, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict20\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n",
      "Ultralytics YOLOv8.1.34 🚀 Python-3.12.2 torch-2.3.0+cpu CPU (11th Gen Intel Core(TM) i5-1135G7 2.40GHz)\n",
      "YOLOv10x summary (fused): 503 layers, 31738160 parameters, 185184 gradients, 170.6 GFLOPs\n",
      "\n",
      "image 1/1 C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\images\\2.png: 384x640 15 67s, 1346.1ms\n",
      "Speed: 4.0ms preprocess, 1346.1ms inference, 6.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\predict21\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "# Run inference on an image with YOLOv8n\n",
    "image_num = 0\n",
    "\n",
    "for image_num in range(3):\n",
    "    !yolo predict model=C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\weights\\yolov10x.pt source=C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\images\\{image_num}.png"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yolo predict model=C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\runs\\mlflow\\237155232739678261\\b1c5387822dc45f69a8a4018b356ba40\\artifacts\\weights\\best.pt source=\"C:\\Users\\SaiRamPenjarla\\Desktop\\Project\\YOLOV10\\archive\\images\\29.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
